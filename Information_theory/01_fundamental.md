# 01. Fundamentals of Information theory

## 1. Quantity of information

- 정보량(Information quantity, **I**) : 그 정보를 얻었을 때 느끼는 놀라움의 정도. 즉 희박한 확률로 일어날 사건이 발생하면 놀라움이 크므로 정보량이 크다고 본다.
    + 조건1, 확률이 작으면 정보량이 크다: `P(x1) > P(x2)` => `I(x1) < I(x2)`
    + 조건2, 독립인 사건이라면 정보량 단순 덧셈 성립: `I(x1x2) = I(x1) + I(x2)`
- 수식 유도 : `I(x) = -log2(P(x))`
    + `I(x) = 1 / P(x)` : 조건1에 따라서 정보량을 확률의 역수로 만든다.
    + 하지만 위 경우엔 조건2 가법성이 성립되지 않는다. A, B 사건이 각각 0.5, 0.25의 확률일 때 정보량은 2, 4의 합은 6인데, joint probability인 0.125의 정보량은 8이다. 6과 8은 다르다.
    + 그래서 log를 씌워서 `log(1/P(x))`를 정보량으로 활용한다. 정보이론에선 주로 밑을 2로 사용. 위의 케이스에서 정보량은 1, 2이고 합은 3이며, joint probability의 정보량 역시 3이다.

## 2. Entropy

- Entropy(**H**) : 모든 사건들의 정보량의 기대값. 모든 사건들에 대해서, 정보량에 그 정보량이 일어날 확률을 곱한 값들을 다 더한다.
- `H(p) = H(X) = Sum( P(x) * log2(1/P(x)) )`
