{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree\n",
    "\n",
    "## 1. 기본\n",
    "\n",
    "![iris](http://scikit-learn.org/stable/_images/sphx_glr_plot_iris_0013.png)\n",
    "\n",
    "- 이전의 classifier들은 linear function으로 데이터를 분류했다. non-linear 데이터에 대해선 SVM의 kernel function을 활용해 차원을 올려서 linear로 분류했다.\n",
    "- Decision tree는 non-linear 데이터를 특히 잘 분류할 수 있다. Decision tree에선 feature를 attribute라고 주로 칭하는데 이 attribute 별로 조건을 설정해서 데이터를 쪼개나간다. 만약 feature가 공부 시간이라면 10시간 이상인지 미만인지로 조건을 주는 식이다.\n",
    "- 주로 분류했을 때 area는 위 이미지처럼 사각형 꼴로 각지게 구분된다.\n",
    "- Decision tree의 장점은\n",
    "    + kernel을 대체할 수 있다.(대표적으로 non-linear XOR 케이스)\n",
    "    + 사람이 이해하고, 해석하기에 좋다. 어떤 조건에 따라 분류되었는지, 분류 순서가 어떻게 되었는지 쉽게 파악이 가능하다.\n",
    "    + Ensemble learning을 하기에 좋다.\n",
    "    + 학습은 몰라도(중요하지 않다) 예측하는 작업은 굉장히 빠르다. O(height of tree)\n",
    "\n",
    "## 2. 알고리즘1 : ID3와 classification\n",
    "\n",
    "\"Iterative Dichotomiser 3\"의 약자이다.\n",
    "\n",
    "### 2.1 어떤 attribute를 먼저 사용할 것인가\n",
    "\n",
    "가장 좋은 attribute를 찾아서 데이터를 나눌 때 먼저 사용해야한다.\n",
    "\n",
    "#### 2.1.1 Entropy\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum_{c \\in C}p(c)\\log{(p(c))} \\\\\n",
    "I = H(S) - \\sum_{i \\in \\{L,R\\}}{|S^i| \\over |S|}H(S^i)\n",
    "$$\n",
    "\n",
    "- IG(Information Gain)와 Entropy를 활용해서 가장 좋은 attribute를 찾는다. 구하는 공식은 다음과 같다.\n",
    "- Entropy : 위 식에서 `H(S)`이고 복잡도를 의미한다. 값이 높을수록 복잡도가 높다. attribute가 2개일 때 Entropy의 변화 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd68f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate_probabilities(*args):\n",
    "    total = sum(args)\n",
    "    return [el / total for el in args]\n",
    "\n",
    "def calculate_entropy(*args):\n",
    "    if len(args) == 0:\n",
    "        raise ValueError('No arguments error')\n",
    "    probs = calculate_probabilities(*args)\n",
    "    result = 0\n",
    "    for el in probs:\n",
    "        if el == 0: continue\n",
    "        result -= el * math.log(el, 2)\n",
    "    return result\n",
    "\n",
    "x1 = np.arange(0, 1.001, 0.01)\n",
    "x2 = 1 - x1\n",
    "\n",
    "y = [calculate_entropy(x1[i], x2[i]) for i in range(len(x1))]\n",
    "plt.plot(x1, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IG(Information Gain) : 어떤 attribute를 선택했을 때 얻게되는 entropy의 감소 정도를 수치로 나타낸다.\n",
    "    + 위 공식의 `I`에 해당하는 부분이고\n",
    "    + `H(S)`는 현재 노드의 복잡도,\n",
    "    + $|S|, |S_i|$는 전체 example의 개수와, i label의 example 개수를 말한다.\n",
    "- 각 attribute에 대해서 모두 IG를 구하고 가장 높은 attribute을 선택한다.\n",
    "- IG를 구하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1058468751414936"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_IG(parent, *children):\n",
    "    p_en = calculate_entropy(*parent)\n",
    "    c_en = []\n",
    "    for child in children:\n",
    "        c_en.append(sum(child)/sum(parent) * calculate_entropy(*child))\n",
    "    return p_en - sum(c_en)\n",
    "\n",
    "calculate_IG([10, 20], [3, 14], [7, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Gini index\n",
    "\n",
    "$$\n",
    "G = \\sum_{j=1}^{c}P(j)(1-P(j)) = 1 - \\sum_{j=1}^{c}P(j)^2 = 1 - \\sum_{j=1}^{c}({n_j \\over n})^2\n",
    "$$\n",
    "\n",
    "- Entropy 말고 gini impurity를 활용해서 attribute를 선택할 수도 있다.\n",
    "- 위 공식대로 gini를 구하고 entropy가 아닌 gini를 활용한 Information gain을 구해서 활용한다.\n",
    "\n",
    "### 2.2 기준 정해서 나누기\n",
    "\n",
    "- 찾은 attribute에 대해서 적절한 기준을 설정하고 분리\n",
    "- 특정 값을 기준으로 나누기도하고, 명목변수로 나눌 수도 있다.(남자인가 아닌가)\n",
    "\n",
    "### 2.3 데이터를 언제까지 분리할 것인가\n",
    "\n",
    "- 두 가지 조건 중 하나를 만족하면 분리를 멈춘다\n",
    "    + node의 entropy가 0일 때, 즉 해당 노드의 example들이 하나의 y 레이블을 가질 때, pure할 때 멈춘다.\n",
    "    + 모든 attribute를 다 썼을 때 멈춘다.\n",
    "- 가능한 끝까지 분리하는 것이기 때문에 overfitting이 무조건 발생하게 되어있다. 이것은 아래에 추가로 설명할 Ensemble learning 방식으로 극복한다.\n",
    "\n",
    "### 2.4 Overfitting 피하기\n",
    "\n",
    "![ID3-overfitting](http://slideplayer.com/8111708/25/images/34/Overfitting+As+ID3+adds+new+nodes+to+grow+the+decision+tree%2C+the+accuracy+of+the+tree+measured.+over+the+training+examples+increases+monotonically..jpg)\n",
    "\n",
    "- Decision tree의 가장 큰 문제점은 overfitting : ID3 알고리즘은 attribute를 다 소모하거나, 모든 노드의 example들이 pure해질 때까지 노드를 추가해나가기 때문에 overfitting이 발생한다.\n",
    "- node가 많아질수록 training data에 대한 accuracy는 높아지지만 test에 대해선 낮아진다.\n",
    "- 위와 같은 문제를 해결하기 위해 아래 두 가지 방법을 사용한다.\n",
    "\n",
    "#### 2.4.1 Setting minimum IG\n",
    "\n",
    "- IG(Information Gain)이 크지 않은 노드는 더 이상 분리하지 않는다.\n",
    "- 예를 들어 10,000개 데이터 중에서 9,995개가 A고, 5개만 B라면 더 이상 분리하지 않는다.(즉 minimum IG를 설정하는 것)\n",
    "\n",
    "#### 2.4.2 Pruning\n",
    "\n",
    "- 일단 ID3 알고리즘으로 가능한 최대 크기로 tree를 만든다.\n",
    "- 데이터셋을 train, validation, test 셋으로 나눈다.\n",
    "    + 100개 데이터 -> 50 / 30, 20\n",
    "    + validation이 필요한 이유는 y label이 없는 test 데이터셋에 decision tree를 적용하기 전에 tree가 제대로 동작하는지 확인하기 위해서다.\n",
    "    + 아래 추가로 설명하듯이 여러가지의 sub tree 중 하나를 골라야하기 때문\n",
    "- 전체 tree에서 subtree를 하나씩 없애본다. parent node만 남기고 child node를 없애보는 것\n",
    "- subtree를 없애본 모든 시도에 대해서 각각을 validation 데이터셋에 적용해보고 accuracy를 측정한다.\n",
    "- 가장 높은 accuracy를 보인 subtree를 선택\n",
    "\n",
    "## 3. 알고리즘2 : Regression\n",
    "\n",
    "### 3.1 Loss 설정\n",
    "\n",
    "$$\n",
    "\\text{RSS} = \\sum_{j=1}^J\\sum_{i \\in R_j}(Y^{(i)} - \\hat{Y_{R_j}})^2\n",
    "$$\n",
    "\n",
    "- P개의 feature($X_1,X_2,...,X_P$)로 구성된 공간을 J개의 겹치지 않는 구역($R_1,R_2,...,R_J$)으로 나눈다.\n",
    "- $R_J$ 구역의 예측값은 그 구역의 mean 값이다\n",
    "- 구역별로 속한 데이터들의 Y값과 구역의 예측값을의 차를 제곱해서 모두 더하는 RSS를 최소화하는 구역들을 찾는다.\n",
    "\n",
    "### 3.2 구역 찾기\n",
    "\n",
    "- 위 Loss를 최소화하는 구역은 한 번에 찾을 수 없기 때문에 불완전하더라도 다음 방식을 사용한다.\n",
    "- top-down, greedy approach -> recursive binary splitting\n",
    "    + top-down: P개의 feature로 이뤄진 공간을 위에서부터 아래로 쪼개나가는 방식(트리의 모양을 생각)\n",
    "    + Greedy: 현재 상태에서 가장 최선의 선택을 하는 것. 첫 번째 쪼개는 feature가 최선이 아니더라도 그 순간에서 최선의 선택을 해나간다. 이후의 조합에 대해선 신경쓰지 않겠다는 의미\n",
    "- 순서\n",
    "    + feature 하나를 정하고, RSS를 가장 크게 줄이는 cutpoint를 정한다.\n",
    "    + cutpoint로 구역을 두 개 나누고, 나눠진 구역 모두에서 계속 같은 작업을 반복한다.\n",
    "    + 반복 종료 조건: 한 구역에 속한 데이터가 특정 개수 이하일 때, RSS가 더 이상 작아지지 않을 때까지\n",
    "\n",
    "### 3.3 Cost complexity pruning\n",
    "\n",
    "- 3.2의 방법까지만 하면 overfitting 문제 발생할 수 있다. 데이터에 대한 이해 없이 반복 종료 조건을 설정하면 굳이 더 이상 쪼갤 필요 없는데도 계속 데이터를 쪼개나갈 수 있기 때문이다.\n",
    "- 그래서 Pruning, 가지치기 방법을 사용한다.\n",
    "- 순서\n",
    "    + 설정한 반복 종료 조건에 맞춰서 일단 최대한 크게 트리를 만든다. 이 트리를 $T_0$라 부른다.\n",
    "    + 가지를 친다. 즉 가장 데이터를 잘 분류하는, RSS가 가장 작은 subtree를 구한다.\n",
    "\n",
    "$$\n",
    "\\underset{T}{\\arg\\min}_T \\sum_{m=1}^{|T|}\\sum_{i \\in R_m}(Y^{(i)} - \\hat{Y_{R_j}})^2 + \\alpha|T|\n",
    "$$\n",
    "\n",
    "> `|T|` : 구역의 개수, 즉 terminal node(leaf)의 개수를 의미\n",
    "\n",
    "- 위 공식처럼 가지의 개수를 페널티로 줘서 가장 적합한 subtree를 구한다.\n",
    "- 먼저 알파 값은 적당한 양수 값을 하나 정해서 고정하고 일단 적합한 |T|부터 찾는다.\n",
    "- 알파값은 나중에 k-fold cross-validation을 통해 prediction-error를 최소화시키는 값으로 정한다.\n",
    "    + 데이터를 train, validation set으로 나눈다.\n",
    "    + 알파를 변경해가며 모든 데이터셋에 대한 에러값을 계산하고 각각의 k-fold 셋마다 sum한다.\n",
    "    + 알파값 중 k-fold의 모든 데이터셋에 대한 에러의 합이 가장 적은 것을 선택\n",
    "\n",
    "## 4. Random forest\n",
    "\n",
    "Decision tree를 단일로 사용하면 accuracy가 낮다. 이를 극복하기 위해 다음 Bagging과 Random Subspace 방법을 모두 사용하고 그것을 Random forest라고 한다.\n",
    "\n",
    "### 4.1 Bagging\n",
    "\n",
    "- Bootstrap Aggregating의 합성어\n",
    "    + Bootstrapping: 원본 데이터셋에서 일정 부분을 여러번 무작위 복원추출하는 것\n",
    "    + Aggregating: 여러 트리를 종합해서 majority voting으로 결정하는 것\n",
    "- 방식\n",
    "    + training data가 n개가 있을 때, 랜덤하게 k개를 뽑는 작업을 b번 반복한다.\n",
    "    + 즉 k 크기의 데이터 셋 (X_b, Y_b) 쌍이 b개 생긴다.\n",
    "    + 이 데이터셋에 ID3 알고리즘을 적용해서 b개의 tree를 생성한다.\n",
    "    + 새로운 데이터가 들어오면 b개의 tree를 동시에 사용해서 다수결(majority vote) 방식으로 분류한다.\n",
    "- Bias-Variance Trade Off\n",
    "    + Bias가 높으면 예측치가 실제값과 비교해 부정확함. 낮을수록 좋다.\n",
    "    + Variance가 높으면 특정 데이터셋에 너무 overfitting된 것\n",
    "    + Bagging 방식은 bias를 낮게 유지하면서 variance를 낮출 수 있다. 즉 accuracy를 높일 수 있다.\n",
    "\n",
    "### 4.2 Random subspace\n",
    "\n",
    "- 원래는 노드를 나눌 때 IG가 최대가 되는 attribute를 선택해서 나눈다.\n",
    "- 하지만 굉장히 강력한 attribute가 있을 경우 Bagging 방식을 이용해봤자 다들 비슷하게 되거나 그 attribute 방향으로 모델이 치우쳐질 수 있다.\n",
    "- 그래서 attribute를 IG와 관계없이 랜덤으로 선택한다. Feature Bagging 방식이라고도 불린다.\n",
    "- 랜덤으로 선택하는 방법은 전체 P개의 feature를 모두 사용하는 것이 아니고 P보다 작은 m개의 feature만 사용하는 것. 이 때 m은 $\\sqrt P$ 또는 $floor(\\log{P} + 1)$ 에 근사하는 값으로 한다.\n",
    "\n",
    "### 4.3 Boosting\n",
    "\n",
    "- Bagging과 유사하지만 조금 다른 개념\n",
    "- 분류\n",
    "    + Ensemble learning: 동일한 전체 데이터셋에 대해 다양한 분류기를 사용한 후 voting\n",
    "    + Bagging: 전체 데이터셋에서 랜덤 샘플링한 다음 각각 샘플에 대해 분류기를 사용한 후 voting\n",
    "    + Boosting: 전체 데이터셋에서 랜덤 샘플링하는 것은 Bagging과 동일\n",
    "        + 다만 Bagging은 각각의 랜덤 샘플에 대해 서로 독립적으로 학습을 하지만\n",
    "        + Boosting은 순차적으로 첫 샘플의 가중치를 다음 샘플에 적용한 후 학습한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
