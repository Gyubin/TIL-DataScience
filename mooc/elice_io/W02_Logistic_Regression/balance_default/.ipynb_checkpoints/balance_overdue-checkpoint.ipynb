{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## 1. 기본\n",
    "\n",
    "### 1.1 개념\n",
    "\n",
    "- 의미: '독립변수 X를 바탕으로 종속변수 Y를 예측한다'는 점에선 Linear regression과 동일하다. 다른 점은 종속변수 Y가 명목변수, 즉 classification에서 class를 의미한다. 즉 어떤 데이터를 A인지 B인지를 판별\n",
    "- 수식의 결과로 \"이 데이터는 A다!\"를 나타낼 수 없으므로 확률을 사용한다. A, B가 될 확률을 각각 계산하고 0.5 이상이면 positive 판별을 한다. 두 개일 때라서 0.5고 3개 이상(multinomial logistic regression)이면 가장 높은 확률을 가진 class로 판별한다.\n",
    "- Linear regression만을 쓰면 계산 안됨 : Y를 확률, X를 feature로 해서 Linear regression을 해보면 Y값이 0 이하인 경우가 나온다. 확률이 음수인 경우는 없으므로 Y 값을 0에서 1 사이로 제한을 해줄 필요가 있다.\n",
    "- Logistic function(or Sigmoid function): 제한을 할 때 사용하고 이를 활용한 regression을 Logistic regression이라 한다. 정확히 말하면 Logistic function 중 standard 형태를 Sigmoid function이라 하고 이 형태를 사용하는 것. logistic function의 정의와 속성은 다음과 같다.\n",
    "\n",
    "\\begin{align}\n",
    "g(x) = {1 \\over 1 + e^{-x}} \\\\\n",
    "{d \\over dx}g(x) = g(x)(1 - g(x)) \\\\\n",
    "1 - g(x) = g(-x)\n",
    "\\end{align}\n",
    "\n",
    "> Logistic function 대신 CDF를 쓸 수도 있는데 이 때는 Probit regression이라고 한다.\n",
    "\n",
    "### 1.2 Decision boundary\n",
    "\n",
    "- Decision boundary: Yes와 No를 가르는 기준이 되는 함수, 즉 Classifier이자 Hyperplane이다.\n",
    "\n",
    "    $f(x) = b_0 + b_1x$\n",
    "\n",
    "    $g(x) = {1 \\over 1 + \\exp(-f(x))} = {1 \\over 1 + \\exp(-(b_0 + b_1x))}$\n",
    "    \n",
    "    $P(Y=Yes) = g(f(x)) = {1 \\over 1 + \\exp(-f(x))} = {1 \\over 1 + \\exp(-(b_0 + b_1x))}$\n",
    "    \n",
    "    $P(Y=No) = 1 - P(Y=Yes) = 1 - g(f(x)) = g(-f(x)) = {1 \\over 1 + \\exp(b_0 + b_1x)}$\n",
    "\n",
    "- Log odds 사용한다. $log \\frac{P(Y=Yes)}{P(Y=No)}$ 식이 0보다 크면 yes다.\n",
    "- 위 Log odds를 풀어쓰면 결국 **f(x)**로 유도되고 0보다 크면 Yes다.\n",
    "\n",
    "    $log \\frac{g(f(x))}{g(-f(x))}$\n",
    "    \n",
    "    $log \\frac{1 + exp(f(x))}{1 + exp(-f(x))}$\n",
    "    \n",
    "    $log \\exp(f(x))$\n",
    "    \n",
    "    $f(x)$\n",
    "\n",
    "### 1.3 Squared loss\n",
    "\n",
    "- Decision boundary는 결국 `f(x)`, `b0 + b1*x`가 되는 것을 확인했다. 이제 가장 적합한 b0, b1을 알아내야한다.\n",
    "- Linear regression에서 사용했던 Squared loss를 쓰면 어떻게 될까. 실제 Y값과 `g(f(x))`의 값의 차이를 제곱한 것을 모두 합한 RSS의 추이를 구해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10828f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def f(x, coef):\n",
    "    return coef * x\n",
    "\n",
    "def g(x, coef):\n",
    "    return 1 / (1 + math.exp(-f(x, coef)))\n",
    "\n",
    "X_rss = np.arange(-50, 50, 0.1)\n",
    "Y_rss = np.array([0 if x < 0 else 1 for x in np.arange(-50, 50, 0.1)])\n",
    "\n",
    "costs = []\n",
    "for coef in np.arange(-1, 1, 0.01):\n",
    "    c = 0\n",
    "    for i in range(len(X_rss)):\n",
    "        c += (Y_rss[i] - g(X_rss[i], coef)) ** 2\n",
    "    costs.append(c)\n",
    "\n",
    "plt.scatter(np.arange(-1, 1, 0.01), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `f(x)`는 intercept가 없는 단순한 `y = b*x` 꼴을 취했다. 그리고 coefficient의 변화에 따라 RSS가 어떻게 달라지는지 그래프를 그렸다.\n",
    "- coefficient b가 음수로 갈수록 RSS는 커지고, 양수로 갈수록 RSS는 작아진다.\n",
    "- 최적의 b를 찾아야하는 상황에서 cost가 수렴하지 않는 non-convex 그래프이고 Gradient descent 알고리즘을 적용해서 구할 수 없다.\n",
    "- 직관적으로 살펴보면\n",
    "    + b가 음의 방향으로 커진다는 것은 0, 1 값으로 이루어진 Y와 완전히 반대 방향으로 그래프가 그려진다는 의미이고 cost가 커지는 것이다.\n",
    "    + 반대로 양의 방향은 Y의 값을 정확히 맞추는 방향으로 그래프가 급격히 기울어지는 것이며 cost가 낮아진다.\n",
    "    + 즉 기울기의 절대값이 커지면 Yes, No의 경계면에서 굉장히 급격하게 거의 y축과 평행으로 솟구치는 것이고 계단함수와 유사하게 된다.\n",
    "    + 그렇게되면 기울기가 커지면 커질수록 좋은 것이고 수렴도 안 할 뿐더러, 현재 1개의 feature라고 했을 때 hyperplane은 특정 스칼라값의 점이 되는데 결국 `intercept / infinite`이 되어 0이 된다. 말이 안된다.\n",
    "- 결론은 쓸 수 없다라는 것.\n",
    "\n",
    "### 1.4 Maximum likelihood\n",
    "\n",
    "> likelihood의 개념은 [김진섭님의 글](http://rstudio-pubs-static.s3.amazonaws.com/204928_c2d6c62565b74a4987e935f756badfba.html)을 보고 이해했다. 최고로 자세하고 체계적인 설명!\n",
    "\n",
    "- Squared loss 대신 Likelihood를 사용하고 최대화하는 방향으로 b0, b1을 계산한다.\n",
    "    + 정의: 어떤 확률변수에서 표집한 값들을 토대로 그 확률변수의 모수를 구하는 방법. 또는 어떤 모수가 주어졌을 때 원하는 값들이 나올 가능도를 최대로 만드는 모수를 선택하는 방법을 말한다.\n",
    "    + 계산 방법: 모든 데이터에 대해서 각각 일어날 확률들을 모두 곱한 값이 likelihood이고 최대화한다.\n",
    "    + 다음 수식의 값을 최대화\n",
    "    \n",
    "$$\n",
    "\\prod_{i=1}^NP(Y\\ |\\ X_i,\\beta_0,\\beta_1)\n",
    "$$\n",
    "\n",
    "- $L = y_ig(f(x)) + (1 - y_i)(1 - g(f(x)))$\n",
    "    + Y가 Yes냐 No냐에 따라 확률 식이 `g(f(x))` 또는 `1 - g(f(x))` 으로 달라진다. b0, b1 값을 계산할 때 매번 수식을 바꾸기는 귀찮으므로 수식을 위처럼 각 수식 앞에 해당 데이터의 y값(yi)을 곱해주는 것으로 변경한다.\n",
    "    + Y가 Yes일 때는 y를 1, No일 때는 y를 0으로 하면 위 식으로 Y가 Yes든 No든 적용 가능해진다.\n",
    "- Maximum log likelihood: log를 씌우면 곱셈을 덧셈으로 바꿀 수 있어서 연산이 쉬워지고 작은 값들에 대한 연산도 가능해진다.\n",
    "    + $\\log(L) = y_i\\times\\log(g(f(x))) + (1 - y_i)\\times\\log(1 - g(f(x)))$\n",
    "- 여기서 -1을 곱하게 되면 최소값을 구하는 문제로 바뀐다.\n",
    "    + $\\text{argmin}_{b0, b1}\\ -\\log L(b0,b1|X, Y)$\n",
    "- Linear regression에서처럼 Gradient descent 알고리즘을 활용해서 최적의 b0, b1 값을 찾는다.\n",
    "- income이라는 feature를 추가해서 b0, b1, b2를 구하게 되더라도 decision boundary는 f(x)로 같다.\n",
    "\n",
    "### 1.5 데이터\n",
    "\n",
    "```\n",
    "   Unnamed: 0  default  student      balance       income\n",
    "0           1        0        0   729.526495  44361.62507\n",
    "1           2        0        1   817.180407  12106.13470\n",
    "2           3        0        0  1073.549164  31767.13895\n",
    "3           4        0        0   529.250605  35704.49394\n",
    "4           5        0        0   785.655883  38463.49588\n",
    "```\n",
    "\n",
    "- 자료\n",
    "    - `default` : 파산 여부\n",
    "    - `student` : 학생 여부\n",
    "    - `balance` : 자산\n",
    "    - `income` : 수입\n",
    "- 학생 여부, 자산, 수입 데이터를 기반으로 파산 여부를 예측해본다.\n",
    "- 나중에 검증할 때는 K-fold cross validation 기법을 활용\n",
    "    + k가 5라면 전체 데이터를 5개로 쪼개고 4개는 학습, 1개는 test로 활용한다.\n",
    "    + 5개가 각각 test를 한 번씩 해보도록 돌려가면서 같은 모델을 테스트.\n",
    "\n",
    "## 2. Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent variable: Balance\n",
      "Linear Coefficients: [ 0.00061851]\n",
      "Linear Intercept: -0.2849174862797101\n",
      "Logistic Coefficients: [[ 0.00437833]]\n",
      "Logistic Intercept: [-5.58917553]\n",
      "======\n",
      "Score: 0.84328358209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qbinson/.virtualenvs/python-ml/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ce52e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(\"./Default.csv\", header=0)\n",
    "    response_var = 1\n",
    "    y_vec = data.ix[:, response_var].as_matrix().squeeze()\n",
    "    x_vec = data.ix[:, 3].as_matrix().reshape(-1, 1)\n",
    "\n",
    "    one_var_default(x_vec, y_vec)\n",
    "\n",
    "\n",
    "def one_var_default(x_vec, y_vec, rs=108):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_vec, y_vec, test_size=0.2, random_state=rs)\n",
    "\n",
    "    regr_linear = linear_model.LinearRegression()\n",
    "    regr_linear.fit(x_train, y_train)\n",
    "\n",
    "    regr_logistic = linear_model.LogisticRegression()\n",
    "    regr_logistic.fit(x_train, y_train)\n",
    "    \n",
    "    score = regr_logistic.score(x_test, y_test)\n",
    "\n",
    "    print(\"Independent variable: {}\".format(\"Balance\"))\n",
    "    print(\"Linear Coefficients: {}\".format(regr_linear.coef_))\n",
    "    print(\"Linear Intercept: {}\".format(regr_linear.intercept_))\n",
    "    print(\"Logistic Coefficients: {}\".format(regr_logistic.coef_))\n",
    "    print(\"Logistic Intercept: {}\".format(regr_logistic.intercept_))\n",
    "    print(\"======\\nScore:\", score)\n",
    "\n",
    "    x_minmax = np.arange(x_vec.min(), x_vec.max()).reshape(-1, 1)\n",
    "    plt.plot(x_vec, regr_linear.predict(x_vec), color='blue', linewidth=3)\n",
    "    plt.plot(x_minmax, regr_logistic.predict_proba(x_minmax)[:, 1], color='red', linewidth=3)\n",
    "    plt.scatter(x_vec, y_vec, color='black')\n",
    "\n",
    "    plt.xlim((x_vec.min(), x_vec.max()))\n",
    "\n",
    "    # filename = \"default_logit_fig.png\"\n",
    "    # plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제 데이터 분포와 Linear, Logical regression 그래프를 모두 그렸다. X는 balance, Y는 파산 여부(default)이다.\n",
    "- black dots: 실제 데이터 분포다. balance에 대한 파산 여부를 점으로 찍음\n",
    "- blue line\n",
    "    + `plt.plot(x_vec, regr_linear.predict(x_vec), color='blue', linewidth=3)`\n",
    "    + X 값과 Linear regression 모델의 predict 값을 활용해서 직선 긋기\n",
    "- red line:\n",
    "    + `x_minmax = np.arange(x_vec.min(), x_vec.max()).reshape(-1, 1)` : x의 최소값에서부터 최대값까지의 모든 자연수를 벡터화\n",
    "    + `plt.plot(x_minmax, regr_logistic.predict_proba(x_minmax)[:, 1], color='red', linewidth=3)`\n",
    "    + 모든 x 범위의 자연수에 대해서 Logistic regression 모델의 예측값을 그래프로 그림\n",
    "- coefficient, intercept\n",
    "    + Linear에선 regression line의 coefficient와 intercept\n",
    "    + Logistic에선 `b0 + b1*x` 값이 0 이상일 때 Yes로 분류하겠다는 decision boundary를 의미\n",
    "- 예측 정확도\n",
    "    + `score = regr_logistic.score(x_test, y_test)`\n",
    "    + train, test 데이터를 분리하고 학습한 모델을 test 데이터에 적용.\n",
    "- 결론은 이런 상황에서는 Linear regression이 큰 힘을 발휘하지 못한다라는 것. Logistic을 써야한다.\n",
    "\n",
    "## 3. Multiple Logistic Regression\n",
    "\n",
    "simple logistic regression에서와 방식은 똑같고 X 값으로 vector가 아닌 matrix 형태의 데이터를 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indepedent variables: ALL\n",
      "Coefficients: [[ -1.72292013e+00   3.72892638e-03  -7.73316052e-05]]\n",
      "Intercept: [-1.47626442]\n",
      "Accuracy: 0.8507462686567164\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(\"./Default.csv\", header=0)\n",
    "    response_var = 1\n",
    "    y_vec = data.ix[:, response_var].as_matrix().squeeze()\n",
    "    x_mat = data.ix[:, range(2, 5)].as_matrix().reshape(-1, 3)\n",
    "\n",
    "    multi_var_default(x_mat, y_vec)\n",
    "\n",
    "\n",
    "def multi_var_default(x_mat, y_vec, rs=108):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_mat, y_vec, test_size=0.2, random_state=rs)\n",
    "\n",
    "    regr_logistic = linear_model.LogisticRegression()\n",
    "\n",
    "    regr_logistic.fit(x_train, y_train)\n",
    "\n",
    "    score = regr_logistic.score(x_test, y_test)\n",
    "\n",
    "    print(\"Indepedent variables: {}\".format(\"ALL\"))\n",
    "    print(\"Coefficients: {}\".format(regr_logistic.coef_))\n",
    "    print(\"Intercept: {}\".format(regr_logistic.intercept_))\n",
    "    print(\"Accuracy: {}\".format(score))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Multi-class classifier\n",
    "\n",
    "- \"1 대 나머지\"(one-vs-rest) 접근법을 활용한다.\n",
    "- A, B, C, D 클래스가 있으면 A냐 아니냐, B냐 아니냐, C냐 아니냐, D냐 아니냐를 분류하는 4개의 binary classifier를 만든다.\n",
    "- 즉 K개의 클래스가 있으면 K개의 binary classifier를 만드는 것.\n",
    "- 테스트할 데이터를 모든 K개의 binary classifier로 돌려보고 가장 높은 확률이 나오는 class를 선택한다.\n",
    "\n",
    "### 4.1 Library, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    return datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- iris 데이터를 사용한다. 꽃의 특징을 3개의 클래스(setosa, versicolor, virginica)로 구분하고자 한다.\n",
    "- `return_X_y=True` : 데이터셋과 label을 분리해서 리턴하는 옵션\n",
    "\n",
    "### 4.2 Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_classification(X, binary_label):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, binary_label)\n",
    "    return lr.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X : 처음 데이터셋에서 X, y로 분리했을 때 받은 X 데이터다.\n",
    "- binary_label : 판별하고자 하는 대상이냐 아니냐를 1과 0으로 표한 1차원 벡터다. 1일 확률을 다시 벡터에 담아 리턴할 것이다.\n",
    "- Logistic regression을 활용해서 둘 중 하나로 분리한다.\n",
    "- `lr.predict_proba(X)`\n",
    "    + 여기선 편의상 학습한 데이터를 그대로 예측할 때도 사용했다.(원래는 이러면 안됨)\n",
    "    + 0인지, 1인지에 대한 확률이 각 데이터 row마다 계산된다.\n",
    "    + `[[0.1, 0.9], [0.2, 0.8], ...]`의 형태다.\n",
    "- 1인 확률을 리턴해야하니 1번 인덱스 컬럼만 잘라서 리턴한다.\n",
    "\n",
    "### 4.3 Multi-class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_classification(X, y):\n",
    "    label_0 = np.array([1 if x == 0 else 0 for x in y])\n",
    "    label_1 = np.array([1 if x == 1 else 0 for x in y])\n",
    "    label_2 = np.array([1 if x == 2 else 0 for x in y])\n",
    "    \n",
    "    p_0 = binary_classification(X, label_0)\n",
    "    p_1 = binary_classification(X, label_1)\n",
    "    p_2 = binary_classification(X, label_2)\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(p_0)):\n",
    "        tmp = [p_0[i], p_1[i], p_2[i]]\n",
    "        result.append(tmp.index(max(tmp)))\n",
    "    result = np.array(result)\n",
    "    \n",
    "    accuracy = sum(result == y) / len(y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우선 binary label을 각 클래스마다 만든다. 0인지 아닌지, 1인지 아닌지, 2인지 아닌지.\n",
    "- 각각의 binary label을 활용해 binary classification을 시행한다. 해당 데이터셋이 0일 확률, 1일 확률, 2일 확률이 계산되어서 저장될 것이다.\n",
    "- 계산된 확률 벡터들을 row 별로 비교해서 가장 높은 확률값을 result에 저장한다.\n",
    "- accuracy는 전체 데이터에서 잘 맞춘 raw의 개수의 비율이다.\n",
    "\n",
    "### 4.4 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = load_data()\n",
    "    accuracy = multiclass_classification(data[0], data[1])\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Ensemble classifier\n",
    "\n",
    "- 여러 개의 classifier를 함께 사용해서 클래스를 분류하는 방법이다.\n",
    "- 예를 들어 classifier가 3개 있다면 각각에 가중치를 주고, 각 classifier가 분류한 클래스의 확률값에 곱해서 비교한다.\n",
    "\n",
    "|classifier|class1|class2|class3|\n",
    "|-----|-----|-----|-----|\n",
    "|classifier 1|w1 * 0.2 | w1 * 0.5 | w1 * 0.3|\n",
    "|classifier 2|w2 * 0.6 | w2 * 0.3 | w2 * 0.1|\n",
    "|classifier 3|w3 * 0.3 | w3 * 0.4 | w3 * 0.3|\n",
    "|weighted average|0.37|0.4|0.23|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf1 score : 0.91\n",
      "clf2 score : 0.81\n",
      "clf3 score : 0.91\n",
      "ensemble clf score : 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import datasets\n",
    "\n",
    "def Ensemble_clf(clfs, X, w):\n",
    "    probs = [clf.predict_proba(X) for clf in clfs]\n",
    "    result = []\n",
    "    for i in range(len(X)):\n",
    "        tmp = probs[0][i, :] * w[0] + \\\n",
    "            probs[1][i, :] * w[1] + \\\n",
    "            probs[2][i, :] * w[2]\n",
    "        result.append(list(tmp).index(max(tmp)))\n",
    "    return np.array(result)\n",
    "\n",
    "def main():\n",
    "    np.random.seed(123)\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data[:, 1:3], iris.target\n",
    "    \n",
    "    random_idx = np.random.permutation(np.arange(len(y)))\n",
    "    X = X[random_idx]\n",
    "    y = y[random_idx]\n",
    "    \n",
    "    clf1 = LogisticRegression(C = 1)\n",
    "    clf2 = LogisticRegression(C = 0.5)\n",
    "    clf3 = GaussianNB()\n",
    "    clfs = [clf1, clf2, clf3]\n",
    "    \n",
    "    w = [1., 1., 1.]\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    result = []\n",
    "    for train, test in kf.split(X):\n",
    "        score = []\n",
    "        \n",
    "        for clf in clfs:\n",
    "            clf.fit(X[train], y[train])\n",
    "            clf_score = clf.score(X[test], y[test])\n",
    "            score.append(clf_score)\n",
    "            \n",
    "        ensemble_clf_score = np.average(y[test] == Ensemble_clf(clfs, X[test], w))\n",
    "        score.append(ensemble_clf_score)\n",
    "        \n",
    "        result.append(score)\n",
    "        \n",
    "    print('clf1 score : %.2f\\nclf2 score : %.2f\\nclf3 score : %.2f\\nensemble clf score : %.2f' \\\n",
    "          % tuple(np.average(np.asarray(result), axis = 0).tolist()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
