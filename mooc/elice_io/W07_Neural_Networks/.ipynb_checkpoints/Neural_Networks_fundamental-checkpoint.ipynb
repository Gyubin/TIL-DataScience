{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## 1. 기본\n",
    "\n",
    "- 여러개의 뉴런 레이어를 연결해서 결과를 도출한다.\n",
    "- Neuron input, output activation\n",
    "- Notation\n",
    "    + w : connection weights\n",
    "    + b : neuron bias\n",
    "    + g() : activation function\n",
    "\n",
    "$$\n",
    "\\text{input} : a(x) = b + \\sum_i w_ix_i = b + w^Tx \\\\\n",
    "\\text{output} : h(x) = g(a(x)) = g(b + \\sum_i w_ix_i) = g(b + w^Tx)\n",
    "$$\n",
    "\n",
    "## 2. Activation function 종류\n",
    "\n",
    "$$\n",
    "g(a) = sigm(a) = {1 \\over 1 + exp(-a)} \\\\\n",
    "g(a) = tanh(a) = {exp(a) - exp(-a) \\over exp(a) + exp(-a)} = {exp(2a) - 1\\over exp(2a) + 1} \\\\\n",
    "g(a) = reclin(a) = max(0, a)\n",
    "$$\n",
    "\n",
    "- Sigmoid function\n",
    "    + 0~1 사이 값\n",
    "    + Strictly increasing(단조증가)\n",
    "    + Bounded\n",
    "    + 항상 양수\n",
    "- tanh(hyperbolic tangent function)\n",
    "    + -1~1사이 값\n",
    "    + Strictly increasing(단조증가)\n",
    "    + Bounded\n",
    "    + 음수 혹은 양수\n",
    "- ReLU: Rectified Linear Unit\n",
    "    + 미니멈 값은 0, 맥시멈 값은 무한\n",
    "    + Strictly increasing(단조증가)\n",
    "    + Neuron을 sparse하게 만들어서 neural networks의 전체 계산 비용을 줄인다. 위의 Sigmoid, tanh같은 경우엔 매우 작은 수는 나오지만 0은 나오지 않기 때문에 결국 모든 뉴런을 계산하게됨. 하지만 ReLU는 0 값이 나올 확률이 높아서 계산할 필요 없는 뉴런을 많이 만들고 즉 sparse하게해서 계산비용을 줄일 수 있다.\n",
    "\n",
    "## 2. Multi-layer Neural Networks\n",
    "\n",
    "![sigle-hidden-layer](https://www.dtreg.com/uploaded/pageimg/MLFNwithWeights.jpg)\n",
    "\n",
    "- 주로 hidden layer에서 output이 여러개가 되면 W는 매트릭스 형태가 된다.\n",
    "- 각각의 output 값에 따라 input vector의 각 값에 적용되는 weight가 다르기 때문.\n",
    "- 각 layer의 output이 다시 input이 되어서 해당 레이어의 weight vector와 곱해져서 다음 레이어로 넘어감\n",
    "\n",
    "$$\n",
    "a(x) = b^{(1)} + W^{(1)}x \\\\\n",
    "h(x) = g(a(x)) \\\\\n",
    "f(x) = o(b^{(2)} + w^{(2)^T}h^{(1)}x)\n",
    "$$\n",
    "\n",
    "- 위 공식에서 (2) 부분은 2번째 레이어라는 의미\n",
    "- 2번째 레이어의 weight벡터와 1번째 레이어의 activation function을 거친 output들이 곱해지고\n",
    "- 2번째 레이어의 bias와 더해져서 output이 된다.\n",
    "- 2번째 레이어에서 output function을 o라고 표시한 이유는 각 레이어마다 다르게 output function을 선택하는 경우가 많기 때문이다. sigmoid, tanh, ReLU 등을 섞어 쓴다.\n",
    "- 최종결과를 주로 f(x)라고 한다. 전체 Neural Networks의 output\n",
    "\n",
    "지금까지는 binary classification이었지만 digit recognition처럼 multiple classification도 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
