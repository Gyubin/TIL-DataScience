{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## 1. 기본\n",
    "\n",
    "- 여러개의 뉴런 레이어를 연결해서 결과를 도출한다.\n",
    "- Neuron input, output activation\n",
    "- Notation\n",
    "    + w : connection weights\n",
    "    + b : neuron bias\n",
    "    + g() : activation function\n",
    "\n",
    "$$\n",
    "\\text{input} : a(x) = b + \\sum_i w_ix_i = b + w^Tx \\\\\n",
    "\\text{output} : h(x) = g(a(x)) = g(b + \\sum_i w_ix_i) = g(b + w^Tx)\n",
    "$$\n",
    "\n",
    "## 2. Activation function 종류\n",
    "\n",
    "$$\n",
    "g(a) = sigm(a) = {1 \\over 1 + exp(-a)} \\\\\n",
    "g(a) = tanh(a) = {exp(a) - exp(-a) \\over exp(a) + exp(-a)} = {exp(2a) - 1\\over exp(2a) + 1} \\\\\n",
    "g(a) = reclin(a) = max(0, a)\n",
    "$$\n",
    "\n",
    "- Sigmoid function\n",
    "    + 0~1 사이 값\n",
    "    + Strictly increasing(단조증가)\n",
    "    + Bounded\n",
    "    + 항상 양수\n",
    "- tanh(hyperbolic tangent function)\n",
    "    + -1~1사이 값\n",
    "    + Strictly increasing(단조증가)\n",
    "    + Bounded\n",
    "    + 음수 혹은 양수\n",
    "- ReLU: Rectified Linear Unit\n",
    "    + 미니멈 값은 0, 맥시멈 값은 무한\n",
    "    + Strictly increasing(단조증가)\n",
    "    + Neuron을 sparse하게 만들어서 neural networks의 전체 계산 비용을 줄인다. 위의 Sigmoid, tanh같은 경우엔 매우 작은 수는 나오지만 0은 나오지 않기 때문에 결국 모든 뉴런을 계산하게됨. 하지만 ReLU는 0 값이 나올 확률이 높아서 계산할 필요 없는 뉴런을 많이 만들고 즉 sparse하게해서 계산비용을 줄일 수 있다.\n",
    "\n",
    "## 2. Multi-layer Neural Networks\n",
    "\n",
    "![sigle-hidden-layer](https://www.dtreg.com/uploaded/pageimg/MLFNwithWeights.jpg)\n",
    "\n",
    "- 주로 hidden layer에서 output이 여러개가 되면 W는 매트릭스 형태가 된다.\n",
    "- 각각의 output 값에 따라 input vector의 각 값에 적용되는 weight가 다르기 때문.\n",
    "- 각 layer의 output이 다시 input이 되어서 해당 레이어의 weight vector와 곱해져서 다음 레이어로 넘어감\n",
    "\n",
    "$$\n",
    "a(x) = b^{(1)} + W^{(1)}x \\\\\n",
    "h(x) = g(a(x)) \\\\\n",
    "f(x) = o(b^{(2)} + w^{(2)^T}h^{(1)}x)\n",
    "$$\n",
    "\n",
    "- 위 공식에서 (2) 부분은 2번째 레이어라는 의미\n",
    "- 2번째 레이어의 weight벡터와 1번째 레이어의 activation function을 거친 output들이 곱해지고\n",
    "- 2번째 레이어의 bias와 더해져서 output이 된다.\n",
    "- 2번째 레이어에서 output function을 o라고 표시한 이유는 각 레이어마다 다르게 output function을 선택하는 경우가 많기 때문이다. sigmoid, tanh, ReLU 등을 섞어 쓴다.\n",
    "- 최종결과를 주로 f(x)라고 한다. 전체 Neural Networks의 output\n",
    "\n",
    "지금까지는 binary classification이었지만 digit recognition처럼 multiple classification도 있다\n",
    "\n",
    "Multiple일 때는 output function을 softmax function(= normalized exponential function)을 사용한다.\n",
    "exponential term을 normalize한 것. sum으로 나눠준 것. 즉 output function을 통해 나온 각각의 값들의 합이 1이 되도록 해서 확률적 특성을 갖게 한다.\n",
    "가장 큰 값을 가진 것을 input에 대한 정답이라고 결정.\n",
    "\n",
    "(y hat은 우리의 예측값, 그냥 y는 실제값)\n",
    "\n",
    "---\n",
    "\n",
    "L개의 hidden layer가 있다면 총 L+1개의 레이어가 있다는 것\n",
    "\n",
    "layer pre-activation for k > 0 ($h^{(0)}(x) = x$)\n",
    "$$\n",
    "a^{(k)}(x) = b^{(k)} + W^{(k)}h^{(k-1)}(x) \\\\\n",
    "$$\n",
    "\n",
    "hidden layer activation ( k from 1 to L )\n",
    "$$\n",
    "h^{(k)}(x) = g(a^{(k)}(x))\n",
    "$$\n",
    "\n",
    "output layer activation (k = L + 1)\n",
    "$$\n",
    "h^{(L+1)}(x) = o(a^{(L+1)}(x)) = f(x)\n",
    "$$\n",
    "\n",
    "멀티 레이어에서는 학습이 어렵다. estimate해야할 parameter들이 굉장히 많기 떄문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Empirical risk minimization\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{\\text{argmin}}{1 \\over T}\\sum_t l(f(x^{(t)};0),y{(t)}) + \\lambda\\Omega(\\theta)_1\n",
    "$$\n",
    "\n",
    "- 앞부분 : loss function. 숫자 인식의 경우 해당 숫자가 맞느냐 아니냐가 1 또는 0으로 나오는데 학습을 위해서 이를 smoothing해야한다. 그래서 loss function 사용\n",
    "- 뒤의 오메가: Regularizer\n",
    "\n",
    "뉴럴넷에서 가장 많이 사용하는 알고리즘이 SGD(Stochastic Gradient Descent)\n",
    "\n",
    "random으로 theta를 initialize하고 iterative하게 값을 찾아나간다.\n",
    "\n",
    "loss, regualrize term을 더 작은 방향으로 가는지. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측값과 실제값의 차이가 얼마나 작은가.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
