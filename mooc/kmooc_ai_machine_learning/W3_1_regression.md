# KMOOC 인공지능과 기계학습 : W3-1 회귀(regression)

## 1. 지도학습(Supervised Learning)

- 무엇인지 알아내고자 하는 **target 함수 f**를 추정하기 위해
- **데이터셋 D**를 알고리즘의 input으로 집어넣어서
- f와 최대한 가까운 함수 **g**를 만들어낸다.

### 1.1 target 함수: `f : X -> Y`

- 알고리즘으로 학습하고자하는 대상 함수를 타겟 함수라고 함.
- input X가 들어오면 Y로 매핑해주는 함수가 타겟 함수
- 알고리즘은 타겟 함수를 학습하는 역할.
- y가 뭐냐에 따라서 분별 태스크, 회귀 태스크로 나뉘어진다.
    + 분별: y가 이산 값을 가질 때. 사람의 이미지가 인풋으로 들어왔을 때 남자냐 여자냐 같은 경우
    + 회귀: Y가 continuous한 값일 때.

### 1.2 데이터셋 : `D = { (xi, yi) | i = 1, ... , N }`

- 만약 함수 f가 정해져있다면 학습할 필요가 없다. 즉 함수 f는 모르는 것으로 가정한다.
- 대신 이 함수 f를 나타내는 input, output 매핑 데이터값이 알고리즘의 input으로 들어온다.
- 데이터는 N개의 instance로 구성되는데 input, output의 쌍이다.
- 데이터를 활용해서 함수 f가 무엇인지 추정한다. 어떻게 매칭되는지 데이터셋으로 예를 들어줬기 때문에 지도학습이다.

### 1.3 알고리즘

- 알고자하는 f에 가장 가깝도록 우리가 구하는 g를 계속 수정해나간다.
- f를 모르기 때문에 알고리즘의 결과물인 함수 g가 f와 얼마나 비슷한지 알기가 어렵다. 이 때 회귀 태스크를 쓴다.

## 2. 회귀 태스크

### 2.1 기본

- 일반적으로 input variable은 independent variable, output variable은 dependent variable이라고 한다.
- 데이터셋에는 x, y 매칭 값이 있다. `y = f(x)` 로 표현된다면 이상적이겠지만 사실상 그러기가 힘들다. 그래서 데이터셋이 랜덤 노이즈 입실론에 의해 perturbation(변형)된 것이라고 가정하고 `y = f(x) + e` 로 표현한다.(e는 입실론)
- 입실론(e)은 정규 분포(`N(0, 𝛔^2`)를 따른다. 평균적으론 노이즈가 없지만 실질적으론 노이즈가 섞이게 되는.

### 2.2 확률 분포

- 학습 알고리즘(learner): g 함수를 리턴한다. 함수 f를 최대한 가까이 모사하는 g이기를 희망함. 세타 값을 어떻게 세팅하느냐에 따라서 함수 g의 모양이 달라진다.(`g(x|theta)`)
- 정규분포를 따르는 입실론에 의해 노이즈가 발생한다는 가정과, theta에 의해 함수 g의 모양이 달라진다는 가정을 활용해 확률 분포 `p(y|x)`가 나온다.
- `p(y|x) ~ N(g(x|theata), 𝛔^2)` : x가 input으로 들어왔을 때 y의 분포는 정규분포가 되고, 평균은 g 함수의 결과, 분산은 이전 입실론의 분산과 같다.

### 2.3 Log likelihood

![Imgur](http://i.imgur.com/JlHcVIa.png)

- 우도를 최대화하는 세타를 찾아가는 과정이다.(Maximum likelihood estimate of theta)
- 값 구하기: x, y가 동시에 일어날 joint probability 모두를 곱하는 것.
- product rule을 통해서 x, y의 joint probability를 likelihood와 prior의 곱으로 바꾼다. 그리고 이 곱셈을 로그의 덧셈으로 바꾸면 마지막 식이 나온다.
- 각각의 x,y 쌍이 독립적인 것이라 가정하기 때문에 파이 기호를 통해 모든 쌍을 곱할 수 있다.
- 곱하는 이유: x, y는 현재 존재하는 데이터 셋이다. x가 input일 때 y가 output인 데이터가 존재하는데 학습 알고리즘으로 나오는 함수 g를 통해 x가 y로 가는지 확률을 계산하는 것. f를 제대로 모사한다는 것은 이 모든 x의 경우에서 g함수가 y로 가야한다는 의미이기 때문에 모든 확률을 곱해야 한다.
- 마지막 식에서 앞 부분만 세타와 관계있다. 두 번째 부분은 input에 대한 distribution이기 때문에 세타를 조정해도 변함이 없다. 그래서 maximize할 때 고려하지 않아도 된다.
- 그래서 첫 번째 부분만 고려한다. 정규분포를 따르기 때문에 PDF로 풀어쓰면 아래 식과 같다.

![Imgur](http://i.imgur.com/dl874Wf.png)

- 식을 앞부분과 뒷 부분(지수 함수) 부분으로 분할 할 수 있다.
- 앞부분은 i와 관련된 것이 없기 때문에 단순히 같은 수가 N번 곱해진 것이다. 그래서 N이 앞으로 나와서 곱해지고, 로그의 분모 분자를 바꾸어 마이너스 값이 된 것. theta와 관련없기 때문에 역시 우도 최대화에서 무시한다.
- 뒷부분은 계산을 차근차근 해보면 답이 나온다. `e`의 지수가 대괄호 안의 부분이다. 결국 x, y 값이 1에서 N까지 변하며 e의 제곱수가 곱해지는 것이다. 그렇기 때문에 1에서 N까지 i값이 변하면서 모든 값이 더해지게 된다. 로그가 자연로그이므로 e와 상쇄되어 없어지고 결국 대괄호 안의 지수의 모든 합이 결과가 된다. 공통 부분 상수가 앞으로 나오고 시그마 기호를 통해 표현할 수 있다.

![Imgur](http://i.imgur.com/nWhRKG3.png)

- 결국 두 번째 항만 중요하게 된다. 첫 번째 항은 세타와 관계 없으므로 무시하고, 두 번째 항에서 시그마 값 또한 상수이므로 무시를 해서 최종적으로 위 Err 함수가 나오게 된다. **즉 log likelihood를 최대화하는 값은 Err 값이 최소화 되는 것**이다. 마이너스가 붙었으므로.
- Err 함수의 값을 최소화하는 theta를 `least squares estimate`라고 한다. 즉 데이터에 있는 y값과 학습 알고리즘을 통해 나온 f를 모사하려하는 g 함수 값의 차이를 제곱해서 모두 더한 다음에 2로 나눈 값이다.

## 3. 선형 회귀

함수 g가 선형 함수라는 가정했을 때 선형 회귀라고 한다.

### 3.1 기본

- `g(xi | w1, w0) = w1*xi + w0`
- 임의의 선형 함수는 두 개의 파라미터 w1, w2로 임의의 선형 함수를 나타낸다.
- w1은 xi에 곱해질 계수(coefficient)가 되고, w0은 x가 0일 때 y 값을 의미.
- 즉 `g(x) = w1*xi + w0`이고 세타는 벡터 `{w0, w1}`다.
- 구하는 방식
    + Error term을 w1으로 편미분해서 extreme 값을 구한다.
    + Error term을 w0으로 편미분해서 extreme 값을 구한다.
    + 위 두 개의 방정식으로부터 가장 좋은 w1, w0의 값을 얻어낸다.

### 3.2 계산 방법

g(x)를 대입해서 Err term을 다시 써보면 `Err = 1/2 * sigma1~N (yi - w1xi - w0)^2` 이다.

![Imgur](http://i.imgur.com/fQAVWwG.png)

- w1에 대해서 편미분
    + 제곱이 없어지면서 맨 앞에서 곱해졌던 1/2와 상쇄된다.
    + `sigma1~N (-xi)*(yi- w1xi - w0)` 이 된다. 김기응 교수님은 편미분할 때 자연스럽게 w1의 계수 -xi를 바깥으로 묶어냈는데 무슨 규칙이 있나보다. 이 부분을 잘 몰라서 제곱 식을 계산해서 편미분해봤더니 동일한 결과가 나왔다.
    + 그리고 이 식을 0으로 세팅해서(extreme 값이 되는) w1을 구하고싶은거다. `sigma(xiyi)`를 우변으로 넘겨서 좌변을 `w0*sigma(xi) + w1*sigma(xi^2)` 꼴로 변경한다.
- w0에 대해서 편미분
    + 제곱이 없어지면서 앞의 1/2와 상쇄되고 w0의 계수인 -1이 뒤에 곱해진다. 이것을 0으로 세팅해서 정리하면 `w1*sigma(xi) + N*w0 = sigma(yi)`

![Imgur](http://i.imgur.com/Grp1lMb.png)

- 위 식을 matrix로 표현하면 아래처럼 된다. `Aw = b` (w, b는 벡터) 식이므로 원하는 솔루션 w를 구하기 위해선 `w = A^-1 * b`처럼 역행렬을 곱해주면 된다.

![Imgur](http://i.imgur.com/6hbCBit.png)

## 4. 다변량 회귀(Multivariate Regression)

### 4.1 기본

![Imgur](http://i.imgur.com/5F1iD2P.png)

- 단변량 회귀에선 input x가 한 개 값이 들어왔지만 이번엔 벡터 형태로 여러 개가 들어온다.
- Error term을 구하는 방식도 단변량일 때와 비슷하다. w0에 대해서 편미분해서 0으로 두는 방정식 하나 만들고, 나머지도 같은 방식으로 하면 d+1개의 선형 연립방정식이 나온다.

### 4.2 계산

- 이번엔 행렬을 활용해서 해를 구한다.
    + 벡터 w: `w0`부터 `wd`까지 값이 들어있다.
    + 벡터 y: `y1`부터 `yN`까지 값이 들어있다.
    + input 행렬 X: 첫 번째 열은 다 1이다. 각 행은 1 다음부터 x의 input vector가 들어간다. 1행에는 x1 input vector, 2행은 x2 input vector가 들어가는 식. 이 행렬을 `design matrix`라고 부르기도 한다.
- 벡터와 행렬을 미분할 때 성질 몇 가지를 이용한다.
    + 벡터 b를 transpose한 것과 벡터 a를 곱한 것. 즉 벡터 a, b의 내적을 벡터 a로 편미분하면 벡터 b가 나온다. 그런데 내적은 스칼라값이기 때문에 벡터 a의 transpose와 벡터 b를 곱한 것으로 표현할 수 있다. 역시 벡터 a로 편미분한 값은 벡터 b가 나온다.
    + 아래 이미지 내용 추가(특히 맨 아래 것)

![Imgur](http://i.imgur.com/mbU6i32.png)

- Error term은 행렬과 벡터를 통해서 한 번에 계산할 수 있다. 아래 이미지 참조
    + 첫 식: 벡터 2개를 내적한 것으로 표현할 수 있다.
    + 둘째 식: 전치행렬은 덧셈일 경우엔 그대로 `(A + B)T = AT + BT`가 되고, 곱셈일 경우엔 순서가 바뀐다. `(AB)T = (BT)(AT)` 이 성질을 이용해 풀어쓴 것.
    + 셋째 식: 행렬 미분 성질 중 `xT * A * x`가 `Ax + ATx` 로 바뀌는 성질을 이용해서 첫 항이 바뀌고, 벡터 미분할 때 transpose의 뒷 부분만 남는 성질을 이용해서 두 번째 항이 바뀐다. 세 번째 항은 사라짐.
    + 넷째 식: `XTX`, `(XTX)T`는 결국 같은 값이므로 `1/2`과 상쇄되고, 전체 식의 값이 0일 때 벡터 w를 구하면 된다.
    + 다섯째 식: 벡터 w를 좌변에 두고 정리한 식이다.

![Imgur](http://i.imgur.com/XeSbPvA.png)

위 식을 통해 벡터 w를 구할 수 있다. w의 각 원소들의 부호(양, 음)는 매칭되는 x의 값이 긍정적인 영향을 끼치는지, 부정적인 영향을 끼치는지 나타낸다.

## 5. 다항식 회귀(Polynomial Regression)

![Imgur](http://i.imgur.com/8zI9iEv.png)

- univariate 일 때 x의 차수가 여러개일 경우를 말한다. 최고차항의 차수가 k라면 k+1개의 파라미터가 필요하다.
- 계산: multivariate의 계산 방식과 동일하다.
    + x에 대해서 보는게 아니라 multivariate input vector z를 만들어서 이것을 input으로 사용한다.
    + z에는 x의 1차부터 k차까지 원소로 들어간다. 그리고 이걸 활용해서 multivariate linear regression으로 풀어낸다.

## 6. Motor Trend 잡지 데이터 예제

![Imgur](http://i.imgur.com/kyrFg44.png)

차량의 중량이 연비에 어떤 영향을 미치는지 예측할 수 있는 회귀 모델을 만들어보자.

`mpg = w0 + w1 * 중량 + e`
