# KMOOC 인공지능과 기계학습 : W2-2 기초 다변량 확률론

다변량이란 변수가 1개 이상인 경우를 말한다. 학생들의 키와 체중, 자동차의 실린더와 마력처럼 동시에 여러 변수를 비교해서 확률을 계산할 때를 다변량이라고 한다.

## 1. Joint , Marginal

### A. 결합 확률 분포(Joint probability)

쉽게 말하면 `교집합` 혹은 `&&` 연산자다. 두 변수 X1, X2가 동시에 각각 어떤 값을 가질 때 확률을 말한다. 서로 다른 동전 2개를 던지는 경우를 예로 들어보자. 하나는 앞면이 나올 확률이 0.5인 unbiased 동전이고, 다른 하나는 앞면이 나올 확률이 0.8인 biased 동전이다. 아래 표에서처럼 앞앞, 앞뒤, 뒤앞, 뒤뒤의 총 4가지 Joint probability가 존재한다.

|             | X2 = 앞 | X2 = 뒤 |
|-------------|---------|---------|
| **X1 = 앞** |     0.4 |     0.1 |
| **X1 = 뒤** |     0.4 |     0.1 |

- Joint PMF : `p(x, y) = P(X=x, Y=y)` 로 표현하고 이산일 때다.
- Joint CDF : `F(x, y) = P(X <= x, Y <= y)` 로 표현하고 누적일 때.
- Joint PDF : `f(x,y)`로 표현하고 주어진 x, y에 대해서 누적 분포 함수(`F(x,y)`)를 x, y에 대해서 미분하면 된다. 확률 밀도 함수일 때.

### B. 주변 확률(Marginal Probability)

Joint Probability(결합 확률)가 정의되어있을 때 두 변수 중 한 가지의 변화에 대해서만 확률이 어떻게 변하는지 알고싶을 때가 있다. 이것을 주변 확률이라 한다. 즉 다시 말해서 y에 관계 없이 확률변수 x가 가질 확률이 얼마냐를 말한다.

|             | X2 = 앞 | X2 = 뒤 | P(X1) |
|-------------|---------|---------|-------|
| **X1 = 앞** |     0.4 |     0.1 |   0.5 |
| **X1 = 뒤** |     0.4 |     0.1 |   0.5 |
| **P(X2)**   |     0.8 |     0.2 |     1 |

추가된 P(X1), P(X2)가 marginal probability다. 즉 X2의 모든 경우의 수에 대해서 X1일 확률을 모두 더하면 된다. 표를 보면 확률의 합임을 알 수 있다. marginal이란 말은 표에서 실제로 주변부에 적히기 때문에 나온 것이라고 한다. marginal cost(한계 비용)와는 관계 없다.

- 확률 질량 함수(Marginal PMF): 모든 y 값에 대한 joint PMF 값을 더해주면 x에 대한 Marginal PMF가 나온다.
- 누적 분포 함수(Marginal CDF): y가 무한대로 갈 때 joint CDF를 구한다.
- 확률 밀도 함수(Marginal PDF): joint PDF를 y에 대해서 - 무한대에서 + 무한대까지 적분하는 것.

## 2. 조건부 확률, 독립

### A. 조건부 확률(conditional distribution)

- Conditional PMF : `P(y∣x) = p(x,y) / p(x)` 로 표현한다. 즉 x값이 주어져있을 때 Y의 확률이다. 분자에는 x, y의 joint probability, 분모에는 x의 marginal probability다.
- Conditional PDF : PMF와 계산 방식은 같다.
- Product rule : 2개 이상일 때 쓰인다. `P(x,y) = P(y∣x)*P(x) = P(x∣y)*P(y)`
- Chain rule : 3개 이상일 때 쓰인다. `P(x,y,z) = P(z∣x,y) * P(y∣x) * P(x)`

### B. 독립(independence)

두 확률 변수가 서로 영향을 주지 않는 독립적 관계일 때 독립이라고 한다. joint probability가 다음처럼 되면 된다.

- 정의: `P(x, y) = P(x) * P(y)`
- 성질: `P(y) = P(y∣x)` or `P(x) = P(x∣y)`

## 3. 베이즈 규칙(Bayes rule)

### A. 정의 

베이즈 규칙은 실제 현상에 대한 추론적 사고를 할 수 있게 도와준다. 베이즈 규칙은 조건부 확률의 product rule에서 유도했다.

```sh
  P(y∣x)    = P(x∣y)    *  P(y)  / P(x)
# posterior = likelihood * prior / norm const
```

- **posterior**: `P(y∣x)`로 표현하고 우리가 알아내고자 하는 가설이다. 예를 들면 x는 사람들의 얼굴 이미지 데이터이고, y는 특정 사람이다.
- **likelihood** : `P(x∣y)` 로 표현. 특정 사람 y라고 했을 때 얼굴 이미지 x가 어떻게 나올 것이냐.
- **prior(사전확률)** : `P(y)` 로 표현. 사전 지식이나 정보, 조건 없이 그 자체로의 확률이다. 전체 경우에서 특정 사람일 확률이다.
- **Normalizing constant** : `P(x)` 로 표현하고 주어진 이미지 데이터를 의미한다. 우리가 원하는 특정 사람에 대한 정보(y)를 계속 바꾸더라도 이 값은 변하지 않는다. 즉 normalizing constant다.
- 즉 posterior 확률은 likelihood, prior에 비례한다.

### B. 자동차 엔진 제원 판별 예제

R 쉘에서 `mtcars`를 입력하면 관련 데이터프레임이 뜬다.

```
P(CYL = 4∣HP = 100) = 𝛂 * P(CYL = 4) * f(HP = 100 ∣ CYL = 4)
```

- 100마력일 때 실린더가 4개일 확률을 구하는 식이다. 실린더는 전체에서 4, 6, 8개 세 종류만 존재한다.
- `𝛂` : 계산하기 쉽도록 수치를 표준화해주는 normalizing constant다. prior, likelihood를 곱한 값은 매우 작다. 실린더 4, 6, 8개의 경우가 전부이므로 이 중에서 4개인 경우를 비율로 나타낸다면 훨씬 더 쉽게 와닿을 것이다. 즉 HP가 100일 때 실린더가 4, 6, 8인 확률들의 합이 1이 되도록 각 확률들을 scaling해주는 constant가 알파다.

## 4. 기대값과 공분산

### A. 확률 질량 함수 PMF

```sh
# K개의 random variable(확률 변수)이 존재하고, 이를 합쳐서 random vector X라 한다.
𝟀 = [X1, X2, X3, ... , Xk]
```

- 기대값
    + 각 변수의 기대값: `E(X1)`, `E(X2)` 등을 구하는 것은 기대값 구하는 기본 방식과 같다. 각 x값과 그 때의 확률을 곱해서 다 더하면 된다. 예를 들어 `X1 ~ Ber(0.8)` 이라면 E(X1)은 1*0.8 + 0*0.2 를 계산해서 0.8이다.
    + 변수의 곱의 기대값: `E(X1*X2)` 같은 경우를 구하는 것도 비슷하다. 일반 기대값을 구하는 과정에서 x값 대신 X1*X2 값이 들어가는 것이고, 확률은 X1, X2의 joint probability가 된다. 이렇게 구한 확률들을 다 더하면 된다.
    + 정리하면 random vector 각 변수들의 모든 조합을 각 상황의 함수(`g(𝟀)`)에 넣고, 그 결과를 joint probability와 곱해서 다 더하면 되는 것이다. 예를 들어 random variable이 3개(x1, x2, x3) 있고, 각각 1, 2, 3의 값을 가질 수 있다고 하자. 그러면 27가지의 모든 경우의 수(111, 112, 113, ..., 332, 333)를 함수에 넣어 결과값을 도출해낸 다음, joint probability에 곱해서 다 더하면 된다. 이산일 땐 시그마, 연속일 땐 적분하여 모두 더한다.
- 공분산: 확률 변수가 2개 이상일 땐 공분산이고 시그마 기호로 표현
    + `Cov[Xi,Xj] = E[(Xi-E[Xi]) * (Xj-E[Xj])] = E[Xi*Xj] - E[Xi]*E[Xj]` : 기본적으로 Xi에 평균을 뺀 값과 Xj에 평균을 뺀 값을 곱해서 평균을 구한다. 분해하면 두 번째 식이 나온다.
    + 이것을 행렬 개념으로 확장시키면 Xi 대신 벡터X를, Xj 대신 벡터 X의 transpose 형태를 써서 똑같이 나타낼 수 있다.
    + i행 i열일 경우를 생각해보면 변수가 하나일 때 분산을 구하는 식인 `E(X^2) - E(X)^2`가 나온다. 즉 공분산은 일변량 분산을 행렬 형태의 다변량으로 확장시킨 개념이다.
- 공분산 행렬: 변수가 2개일 때 [0,0] 부분은 x의 분산을, [1,1]은 y의 분산을 의미한다. 그리고 [0,1], [1,0]은 x, y의 off diagonal element를 의미하는데 상관계수 정도로 이해하면 되겠다. -1에서 1 사이의 값을 갖는다.

### B. 확률 밀도 함수 PDF

- `f(x;u,∑)` = ![pdf](http://www.extremeoptimization.com/Documentation/media/mvnormalPDF.gif)
- 위 식은 정규분포일 때다. random vector x에 대한 함수이며, ∑는 공분산 행렬을 의미한다. 즉 random vector x가 기대값 벡터 u에서 얼마나 떨어져있는지 구하는 것이고, 앞부분의 나누는 상수는 지수 부분이 모든 x에 대해서 적분을 하면 1이 되도록 하는 normalizing constant다.
- 정규분포일 때 공분산 행렬 예시(출처: 강의자료)

![Imgur](http://i.imgur.com/eY3gPpE.png)
