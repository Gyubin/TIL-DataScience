# KMOOC 인공지능과 기계학습: W2-1 기초 확률론

## 1. 확률 기초

- 표본 공간(sample space): 확률 실험(random experiment)으로 관측될 수 있는 모든 산출(outcome)의 집합이다. 전체 집합은 대문자 오메가(𝛀)로, 원소는 소문자 오메가(𝛚)로 표시한다. `𝛀 = {𝛚1, 𝛚2, 𝛚3, 𝛚4, ...}`
    + 확률의 성립 조건 1: 모든 사건에 대해서 확률이 0 이상일 것
    + 2: 표본 공간의 확률은 1
    + 3: 사건(event)들이 서로 배타적일 때 사건들의 합집합의 확률은 각각의 확률을 더한 값과 같다.
- 사건 공간(event space): 모든 사건의 집합. 모든 사건은 sample space에 포함되고, event space는 sample space의 부분집합이다. `𝓕 = {A1, A2, A3 ...}`
    + event space 만족 조건 1: 공집합이 사건공간 안에 포함되어있을 것
    + 2: 사건 A가 사건공간 안에 있다면, A의 여집합도 사건공간 안에 있을 것
    + 3: A1, A2가 사건 공간 안에 있다면 A1, A2의 합집합도 사건 공간에 들어가 있을 것
- 6면 주사위 예제
    + 표본 공간: `𝛀 = {1, 2, 3, 4, 5, 6}`
    + 사건 공간: 수행자가 어떤 것에 관심있느냐에 따라 달라진다. 먼저 구하고자 하는 사건들을 사건 공간에 집어넣은 후에 사건 공간을 만족하는 조건으로 검증하며 완성하면 된다.
        * 짝수/홀수: `A1={2, 4, 6}`, `A2={1, 3, 5}`를 우선 생각해낼 수 있다. 그리고 조건을 만족시키기 위해 공집합과 오메가(혹은 A1UA2)를 집어넣으면 된다. `𝓕 = {𝜙, A1, A2, 𝛀}`
        * 2 이하: `A1={1, 2}`, `A2{3, 4, 5, 6}`를 우선 생각한 후 나머지를 추가한다. `𝓕 = {𝜙, A1, A2, 𝛀}`
- `∣A1∣` : 사건을 절대값 기호(?)로 감싸면 A1 사건의 원소 개수를 의미한다.

## 2. 확률 변수

- 이산 확률 변수(discrete random variable): 셀 수 있는 것들
    + 동전을 던졌을 때 앞면이 나온 횟수
    + `P(X = k) = P({𝛚: X(𝛚) = k})`
- 연속 확률 변수(continuous random variable): 셀수 없는 굉장히 다양한 사건들
    + 3학년 3반의 몸무게 분포
    + `P(a≤X≤ b) = P({𝛚: a≤X(𝛚)≤b})`

## 3. 확률 분포 함수

- 확률 질량 함수(probability mass function; PMF)
- 누적 분포 함수(cumulative distribution function; CDF)
- 확률 밀도 함수(probability density function; PDF): 연속일 때만 정의가 됨. 주어진 x일 때 미분값. 누적함수 분포의 기울기.

확률분포는 결국 확률 변수에 대해서 확률 값들이 assign된 함수다. 빈도와 상관없이 세가지 조건을 만족하면 얼마든지 확률이라고 할 수 있다.

## 4. 기대값과 분산 공식

- 기대값
    + PMF: 값(x)과 확률의 곱을 모두 합한 값
    + PDF: 역시 PMF와 원리는 같지만 적분해야한다. x값을 정말 잘게 쪼개어 확률과 곱한 값을 더한다.
- 분산: `Var[X] = E[X^2] - E[X]^2`

## 5. 대표적 확률 분포

### A. 이산 확률 분포

- 베르누이 분포(Bernoulli distribution): `X ~ Bern(p)`
    + 베르누이 분포는 성공과 실패 두 개만 존재한다. 즉 x 값은 1(성공), 0(실패) 뿐이다.
    + 매개변수는 p 하나를 받는다. x가 1일 경우인 확률을 의미한다.
    + 예시: 동전이 앞면이냐 뒷면이냐, 웹에서 고객이 광고를 봤을 때 클릭을 할 것이냐 하지 않을 것이냐같은 2가지 상황만 있는 분포
    + x가 1일 확률이 p, 0일 확률이 1-p 일 때 `E[X] = p`, `E[X^2] = p`, `Var[X] = p(1-p)` 이다.
- 이항 분포(Binomial distribution): `X ~ Binom(n, p)`
    + p 매개변수를 받는 베르누이 분포를 n 번 시행하는 뜻이다. 이는 곧 베르누이 n개를 다 합쳐서 더한다는 의미다. `X = X1 + X2 + ... + Xn`
    + 각각의 확률 값은 `px(x) = nCx * p^x * (1-p)^(n-x)` 로 계산한다.
    + `E(X) = np`, `Var[X] = np(1-p)`

### B. 연속 확률 분포

- 균등 분포(Uniform distribution): `X ~ Uni(a, b)`
    + 한 점에서의 확률은 `1/(b-a)`다. 범위 외에서는 모두 0
    + a에서 b까지만 적분하면 된다. 모든 구간 할 필요가 없다.
    + `E[X] = (a+b)/2`, `V[X] = (b^2 + ab + a^2) / 3`
- 균등 분포는 `f(x) = c` 처럼 항상 상수값이 결과로 나오는 것이다. 하지만 f(x)가 일차식일 수도 있고, 이차식일 수도 있다. 이 때 기대값을 구하려면 역시 적분을 해야 하는데 `x`값을 `f(x)`에 곱한 값을 적분해야 한다. f(x)를 적분한 후에 x를 곱하는 것이 아니다. 다시 말해 기대값은 모든 x에 대해 `x*f(x)` 값을 더한 값이다. 그래서 연속확률분포에선 적분을 해야한다. 예를 들면 `f(x)=2x+1`일 때 기대값을 구하려면 `x*f(x)`를 적분해서 원시함수 F(x)를 만들고 이것을 x에 범위에 맞게 `F(b) - F(a)` 하면 결과값이 나온다.
- 정규분포(Normal or Gaussian distribution): `X ~ 𝓝(𝛍, 𝛔^2)`

## 6. 최우도 추정(maximum likelihood estimation)

- 데이터
    + 정규분포임을 가정한다.
    + 주어진 데이터를 잘 설명하는 평균과 분산이 무엇인지, 즉 확률분포가 무엇인지 알아내는 것이 추정이다.
- 우도(likelihood)
    + `L(𝓓; 𝛍, 𝛔)` : 주어진 데이터가 있고, 매개변수의 평균과 표준편차를 가지고 확률 분포를 그렸을 때, 각각 관측값(Xi)의 PDF 값을 모두 곱한 값을 리턴한다. 즉 평균과 표준편차를 가정했을 때 전체 데이터를 모두 볼 확률이 얼마가 되겠느냐는 의미이다.
    + `𝓛(𝓓; 𝛍, 𝛔)` : Log likelihood. likelihood에 로그를 씌운 것이다. L을 약간 삐딱하게 쓰는걸로 기본 우도와 구분한다. 각각의 PDF 값의 로그 값을 모두 더한 값이다. notation을 한다. 즉 각각의 PDF 값의 로그값을 더한 것이다.
    + 왜 likelihood, log likelihood 를 둘 다 쓰느냐. 주로 log likelihood를 쓴다. 왜냐면 데이터를 점점 쌓으면 쌓을수록 우도 값은 엄청나게 빠른 속도로 0에 수렴하게 된다. 그리고 컴퓨터는 작은 숫자는 그냥 0으로 취급해버리기 때문에 좋은 값을 찾을 수가 없다. 대신 로그는 점점 서서히 마이너스 값으로 가기 때문에 정확한 값을 계산하기가 훨씬 수월하다. log likelihood가 최대값이 되는 뮤와 시그마를 찾으면 된다.
- 최우도 추정(Maximum likelihood estimation)
    + 우도를 극대화해주는 뮤와 시그마값을 찾는 것.
    + 각각의 뮤와 시그마에 미분을 해서(시그마는 편미분) 극한값(0)을 가질 때를 알아내면 된다. 
    + 식
    + ![MLE](http://qbinson.com/wp-content/uploads/2016/02/maximum_likelihood_estimation.png)
